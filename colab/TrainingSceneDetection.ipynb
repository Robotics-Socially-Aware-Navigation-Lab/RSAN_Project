{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzl_60DG8t1Y"
      },
      "outputs": [],
      "source": [
        "# ===============================================================\n",
        "# CELL 1 — MOUNT GOOGLE DRIVE AND DEFINE RSAN PROJECT PATHS\n",
        "# ---------------------------------------------------------------\n",
        "# Reuses the same project structure as your original notebook.\n",
        "# - RSAN_ROOT:   /content/drive/MyDrive/RSAN_Project (1)\n",
        "# - MIT_ROOT:    raw MIT Indoor Scenes dataset\n",
        "# - CLS_DATASET_DIR: 10-class processed dataset (train/val)\n",
        "# - INDOOR_MODEL_DIR: where we'll save the new Places365+MIT model\n",
        "# ===============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Root of your RSAN project in Drive (adjust folder name if needed)\n",
        "RSAN_ROOT = Path(\"/content/drive/MyDrive/RSAN_Project\")\n",
        "\n",
        "# Raw MIT Indoor Scenes dataset location (as in your old notebook)\n",
        "MIT_ROOT = RSAN_ROOT / \"datasets\" / \"MIT_Indoor_Scenes\"\n",
        "MIT_IMAGES_DIR = MIT_ROOT / \"indoorCVPR_09\" / \"Images\"\n",
        "TRAIN_LIST = MIT_ROOT / \"TrainImages.txt\"\n",
        "TEST_LIST = MIT_ROOT / \"TestImages.txt\"\n",
        "\n",
        "# indoor classification dataset (already prepared for YOLOv8-CLS)\n",
        "# We'll reuse this for resnet_places365 fine-tuning.\n",
        "CLS_DATASET_DIR = RSAN_ROOT / \"datasets\" / \"indoor_scenes_cls\"\n",
        "\n",
        "# Output directory for trained indoor classifier artifacts\n",
        "INDOOR_MODEL_DIR = RSAN_ROOT / \"models\" / \"indoor_classification\"\n",
        "INDOOR_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"RSAN_ROOT:\", RSAN_ROOT)\n",
        "print(\"MIT_IMAGES_DIR exists:\", MIT_IMAGES_DIR.exists())\n",
        "print(\"Train list exists:\", TRAIN_LIST.exists())\n",
        "print(\"Test list exists:\", TEST_LIST.exists())\n",
        "print(\"CLS_DATASET_DIR exists:\", CLS_DATASET_DIR.exists())\n",
        "print(\"INDOOR_MODEL_DIR:\", INDOOR_MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPVgMTUO_Afh"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 2 — COUNT IMAGES PER CLASS IN 4-CLASS DATASET\n",
        "# ---------------------------------------------------------------\n",
        "# Expects:\n",
        "#   CLS_DATASET_DIR/\n",
        "#       train/\n",
        "#           office/\n",
        "#           hallway/\n",
        "#           classroom/\n",
        "#           lab/\n",
        "#       val/\n",
        "#           office/\n",
        "#           hallway/\n",
        "#           classroom/\n",
        "#           lab/\n",
        "# ================================================================\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "if not CLS_DATASET_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"{CLS_DATASET_DIR} does not exist. \"\n",
        "        \"Make sure your 4-class dataset is created or update CLS_DATASET_DIR.\"\n",
        "    )\n",
        "\n",
        "counts = defaultdict(int)\n",
        "\n",
        "for split in [\"train\", \"val\"]:\n",
        "    split_root = CLS_DATASET_DIR / split\n",
        "    if not split_root.exists():\n",
        "        raise FileNotFoundError(f\"Missing split folder: {split_root}\")\n",
        "    for cls_dir in split_root.iterdir():\n",
        "        if not cls_dir.is_dir():\n",
        "            continue\n",
        "        n = len(list(cls_dir.glob(\"*.jpg\")))\n",
        "        counts[(split, cls_dir.name)] = n\n",
        "\n",
        "print(\"\\nImage counts per class:\")\n",
        "for (split, cls_name), n in sorted(counts.items()):\n",
        "    print(f\"{split:5s} | {cls_name:10s} : {n}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t01cX4vAoE4"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 3 — IMPORT LIBRARIES & SET CONFIG\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 40   # adjust as desired\n",
        "LR = 1e-3         # learning rate for MIT head\n",
        "\n",
        "# Image transforms (Places365 / ImageNet-style normalization)\n",
        "train_tf = transforms.Compose([\n",
        "    # Randomly crop and resize: zooms in/out a bit while keeping 224×224\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "    # Random horizontal flip\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    # Small random rotation (in degrees)\n",
        "    transforms.RandomRotation(10),\n",
        "\n",
        "    # Slight random changes in brightness/contrast/saturation\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.2,\n",
        "        contrast=0.2,\n",
        "        saturation=0.2,\n",
        "    ),\n",
        "\n",
        "    # Convert to tensor\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Normalize with ImageNet / Places stats\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Keep val_tf the same as before:\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwU1-vhTAvVI"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 4 — BUILD DATASETS & DATALOADERS FROM 4-CLASS DATASET\n",
        "# ================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "train_root = CLS_DATASET_DIR / \"train\"\n",
        "val_root   = CLS_DATASET_DIR / \"val\"\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_root, transform=train_tf)\n",
        "val_ds   = datasets.ImageFolder(val_root,   transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "NUM_MIT_CLASSES = len(class_names)\n",
        "\n",
        "print(\"MIT indoor (specialized) classes:\", class_names)\n",
        "print(\"NUM_MIT_CLASSES:\", NUM_MIT_CLASSES)\n",
        "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeLbiVHdA2lP"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 5 — LOAD RESNET50-PLACES365 (AUTO-DOWNLOAD IF NEEDED)\n",
        "# ================================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "PLACES_WEIGHTS_PATH = RSAN_ROOT / \"models\" / \"resnet50_places365.pth.tar\"\n",
        "PLACES_WEIGHTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not PLACES_WEIGHTS_PATH.exists():\n",
        "    print(\"Weights not found, downloading ResNet50-Places365...\")\n",
        "    os.system(\n",
        "        f'wget -O \"{PLACES_WEIGHTS_PATH}\" '\n",
        "        'http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar'\n",
        "    )\n",
        "else:\n",
        "    print(\"Found existing weights at:\", PLACES_WEIGHTS_PATH)\n",
        "\n",
        "def load_resnet50_places365(weights_path: Path):\n",
        "    import torch\n",
        "    import torchvision.models as models\n",
        "\n",
        "    model = models.resnet50(num_classes=365)\n",
        "    checkpoint = torch.load(str(weights_path), map_location=\"cpu\")\n",
        "\n",
        "    # Some checkpoints have 'state_dict', some don't\n",
        "    state = checkpoint.get(\"state_dict\", checkpoint)\n",
        "\n",
        "    # Strip possible 'module.' prefixes\n",
        "    new_state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
        "    model.load_state_dict(new_state, strict=True)\n",
        "    return model\n",
        "\n",
        "resnet_places = load_resnet50_places365(PLACES_WEIGHTS_PATH)\n",
        "resnet_places.eval()\n",
        "print(\"Loaded ResNet50-Places365.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4UddXIpBbHh"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# NEW CELL 6 — MULTI-HEAD MODEL: RESNET50 BACKBONE\n",
        "# ---------------------------------------------------------------\n",
        "# - Shared backbone: ResNet50 up to the final FC\n",
        "# - Head 1: Places365 classifier (original fc, 365 classes)\n",
        "# - Head 2: MIT indoor classifier (N classes from your dataset)\n",
        "# ================================================================\n",
        "\n",
        "import copy\n",
        "\n",
        "class PlacesMITMultiHead(nn.Module):\n",
        "    def __init__(self, resnet_places: nn.Module, num_mit_classes: int):\n",
        "        super().__init__()\n",
        "        self.backbone = resnet_places\n",
        "\n",
        "        # Original final FC: in_features -> 365\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.places_head = copy.deepcopy(self.backbone.fc)\n",
        "\n",
        "        # Replace backbone.fc with identity so backbone(x) returns features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # New head for MIT Indoor classes\n",
        "        self.mit_head = nn.Linear(in_features, num_mit_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)          # [B, in_features]\n",
        "        places_logits = self.places_head(feats)\n",
        "        mit_logits = self.mit_head(feats)\n",
        "        return places_logits, mit_logits\n",
        "\n",
        "model = PlacesMITMultiHead(resnet_places, num_mit_classes=NUM_MIT_CLASSES).to(DEVICE)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hwdsHMsBkcw"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# NEW CELL 7 — FREEZE BACKBONE + PLACES HEAD (NO FORGETTING)\n",
        "# ---------------------------------------------------------------\n",
        "# We want:\n",
        "#   - Places365 predictions identical to original ResNet50-Places365\n",
        "#   - Only MIT indoor head is trained\n",
        "# ================================================================\n",
        "\n",
        "# Freeze all backbone parameters (conv layers + BN, etc.)\n",
        "for p in model.backbone.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Freeze Places365 classifier head\n",
        "for p in model.places_head.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Only MIT head is trainable\n",
        "for p in model.mit_head.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "criterion_mit = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.mit_head.parameters(), lr=LR)\n",
        "\n",
        "print(\"Trainable parameters:\")\n",
        "for name, p in model.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(\"  \", name, p.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2bS-iy3BuEp"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 8 — TRAIN MIT INDOOR HEAD\n",
        "# ================================================================\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        _, mit_logits = model(imgs)  # we only supervise MIT head\n",
        "        loss = criterion(mit_logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = mit_logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def eval_model(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            _, mit_logits = model(imgs)\n",
        "            preds = mit_logits.argmax(1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    return acc, all_labels, all_preds\n",
        "\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"\\n===== Epoch {epoch}/{NUM_EPOCHS} =====\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion_mit, DEVICE\n",
        "    )\n",
        "    val_acc, val_labels, val_preds = eval_model(model, val_loader, DEVICE)\n",
        "\n",
        "    print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
        "    print(f\"Val acc:    {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model (MIT head fine-tuned)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_path = INDOOR_MODEL_DIR / \"resnet_places365_mit_multihead_best11.pth\"\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(\"✅ New best model saved to:\", best_path)\n",
        "\n",
        "print(\"\\nBest validation accuracy:\", best_val_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwaP0ZcxBxq9"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 9 — CONFUSION MATRIX & CLASSIFICATION REPORT\n",
        "# ================================================================\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Reuse val_labels, val_preds from the last epoch\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "print(\"Classification report (MIT head):\\n\")\n",
        "print(classification_report(val_labels, val_preds, target_names=class_names))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"MIT Indoor (10-class) — Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7oVqiQWIp3Z"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 10 — RELOAD BEST MODEL & PREDICT ON NEW IMAGES\n",
        "# ---------------------------------------------------------------\n",
        "# This gives you access to BOTH:\n",
        "#   - Places365 classes  (365-way head)\n",
        "#   - MIT indoor classes (4-way head)\n",
        "# ================================================================\n",
        "\n",
        "# Reload best weights (if not already in memory)\n",
        "best_path = INDOOR_MODEL_DIR / \"resnet_places365_mit_multihead_best.pth\"\n",
        "print(\"Loading best model from:\", best_path)\n",
        "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# If you have a mapping for Places365 class indices -> names, load it here.\n",
        "# For now we'll just output the argmax index for Places365.\n",
        "def predict_image(img_path: Path):\n",
        "    \"\"\"\n",
        "    Run model on a single image and return:\n",
        "      - places_idx: argmax index over 365 classes\n",
        "      - mit_idx: argmax index over your MIT indoor classes\n",
        "    \"\"\"\n",
        "    from PIL import Image\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    x = val_tf(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        places_logits, mit_logits = model(x)\n",
        "        places_probs = F.softmax(places_logits, dim=1)\n",
        "        mit_probs = F.softmax(mit_logits, dim=1)\n",
        "\n",
        "        places_idx = int(places_probs.argmax(1))\n",
        "        mit_idx = int(mit_probs.argmax(1))\n",
        "\n",
        "    return {\n",
        "        \"places_idx\": places_idx,\n",
        "        \"mit_idx\": mit_idx,\n",
        "        \"mit_class_name\": class_names[mit_idx],\n",
        "        \"places_conf\": float(places_probs[0, places_idx]),\n",
        "        \"mit_conf\": float(mit_probs[0, mit_idx]),\n",
        "    }\n",
        "\n",
        "# Example: test on a random validation image\n",
        "from random import choice\n",
        "\n",
        "sample_class_dir = choice(list(val_root.iterdir()))\n",
        "sample_img = choice(list(sample_class_dir.glob(\"*.jpg\")))\n",
        "print(\"Sample image:\", sample_img)\n",
        "\n",
        "pred = predict_image(sample_img)\n",
        "print(\"\\nPredictions:\")\n",
        "print(\"  Places365 index:\", pred[\"places_idx\"], f\"(conf={pred['places_conf']:.3f})\")\n",
        "print(\"  MIT class:\", pred[\"mit_class_name\"], f\"(conf={pred['mit_conf']:.3f})\")\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "display(Image.open(sample_img))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# COMPARE MULTIPLE MODELS ON THE VALIDATION SET\n",
        "# ---------------------------------------------------------------\n",
        "# - Loads each .pth model you specify\n",
        "# - Evaluates MIT head on val_loader\n",
        "# - Prints accuracy + classification report\n",
        "# - Shows confusion matrix\n",
        "# ================================================================\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from pathlib import Path\n",
        "\n",
        "# If you want to upload models from your local machine into this runtime,\n",
        "# you can uncomment these two lines and upload .pth files:\n",
        "#\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # after this, use Path(name) for each uploaded file\n",
        "#\n",
        "# But usually it's easier to put models in INDOOR_MODEL_DIR on Drive\n",
        "# and list them in MODEL_FILES below.\n",
        "\n",
        "def build_empty_model(num_mit_classes: int):\n",
        "    \"\"\"\n",
        "    Recreate the multi-head model architecture and load the\n",
        "    Places365 backbone weights. We'll then load the fine-tuned\n",
        "    weights from a .pth file into this model.\n",
        "    \"\"\"\n",
        "    resnet_places = load_resnet50_places365(PLACES_WEIGHTS_PATH)\n",
        "    model = PlacesMITMultiHead(resnet_places, num_mit_classes=num_mit_classes)\n",
        "    model.to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def evaluate_model_from_path(model_path: Path, label: str = None):\n",
        "    \"\"\"\n",
        "    Load a model state_dict from `model_path`, evaluate it on the\n",
        "    validation set, and show accuracy + confusion matrix.\n",
        "    \"\"\"\n",
        "    if label is None:\n",
        "        label = model_path.name\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Evaluating model: {label}\")\n",
        "    print(\"Path:\", model_path)\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # Build fresh model and load weights\n",
        "    model = build_empty_model(NUM_MIT_CLASSES)\n",
        "    state = torch.load(str(model_path), map_location=DEVICE)\n",
        "    model.load_state_dict(state)\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Forward pass (we only care about MIT head here)\n",
        "            _, mit_logits = model(imgs)\n",
        "            preds = mit_logits.argmax(1)\n",
        "\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "\n",
        "    # Accuracy\n",
        "    acc = (all_labels == all_preds).mean()\n",
        "    print(f\"Validation accuracy: {acc:.4f}\\n\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(f\"Confusion Matrix — {label}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EDIT THIS LIST: models you want to compare\n",
        "# (they should all live in INDOOR_MODEL_DIR or give full Paths)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "MODEL_FILES = [\n",
        "    \"resnet_places365.pth\",   # your current best\n",
        "    # \"resnet_places365_mit_multihead_augmented_best.pth\",  # example extra\n",
        "]\n",
        "\n",
        "# Evaluate each model in the list\n",
        "for fname in MODEL_FILES:\n",
        "    model_path = INDOOR_MODEL_DIR / fname\n",
        "    if not model_path.exists():\n",
        "        print(f\"\\n⚠️ Skipping {fname} (file not found at {model_path})\")\n",
        "        continue\n",
        "    evaluate_model_from_path(model_path, label=fname)\n"
      ],
      "metadata": {
        "id": "wj7GLxWlynHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}