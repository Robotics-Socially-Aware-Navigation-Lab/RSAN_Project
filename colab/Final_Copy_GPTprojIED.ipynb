{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 0 – Install dependencies (YOLO, OpenCV, OpenAI, etc.)\n",
        "!pip -q install ultralytics opencv-python matplotlib pillow pandas imageio openai>=1.40.0 seaborn\n",
        "\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "metadata": {
        "id": "TakdzZu5vEI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 – Load YOLO model (YOLOModel6_withRobo + KEEP_IDS)\n",
        "\n",
        "import os, glob, math\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# KEEP_IDS (filtered COCO classes)\n",
        "KEEP_IDS = [\n",
        "     0, 15, 16, 24, 25,\n",
        "    28, 39, 40, 41, 42, 43, 44, 45, 46,\n",
        "    47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
        "    56, 57, 59, 60, 61, 63, 64, 65, 66,\n",
        "    67, 68, 70, 71, 73, 74, 75, 76,\n",
        "    77, 78, 79, 80, 82, 83, 85, 86, 88,\n",
        "    89, 90, 91,\n",
        "]\n",
        "\n",
        "WEIGHTS_PATH = \"/content/YOLOModel6_withRobo.pt\"\n",
        "assert os.path.exists(WEIGHTS_PATH), f\"Model missing: {WEIGHTS_PATH}\"\n",
        "\n",
        "det_model = YOLO(WEIGHTS_PATH)\n",
        "\n",
        "print(\"Loaded YOLO model from:\", WEIGHTS_PATH)\n",
        "print(\"Number of classes:\", len(det_model.model.names))\n",
        "print(\"Some sample classes:\", list(det_model.model.names.values())[:20])\n",
        "print(\"KEEP_IDS:\", len(KEEP_IDS), \"classes\")\n"
      ],
      "metadata": {
        "id": "81icWfqIvFBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 – Mount Google Drive and list indoor videos\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/indoor_videos\"\n",
        "videos = sorted(glob.glob(os.path.join(VIDEO_DIR, \"*\")))\n",
        "\n",
        "print(\"Found videos:\", len(videos))\n",
        "for v in videos:\n",
        "    print(v)\n"
      ],
      "metadata": {
        "id": "7VIwwq6yvHL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 – YOLO video processor using positions (center, w, h, conf)\n",
        "#           + frame skipping + monitor-specific threshold\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# labels we treat as \"computer monitor / tv\"\n",
        "MONITOR_LABELS = {\"computer monitor\", \"tv\", \"monitor\"}\n",
        "MONITOR_CONF_THRESH = 0.40   # ignore monitor < 40%\n",
        "\n",
        "def run_yolo_video(\n",
        "    model,\n",
        "    video_path,\n",
        "    save_every=10,\n",
        "    conf_thresh=0.25,\n",
        "    keep_ids=None,\n",
        "    frame_skip=5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run YOLO on a video.\n",
        "\n",
        "    Outputs:\n",
        "      - annotated MP4 video\n",
        "      - CSV of frame-level detections with center x/y, width, height, confidence\n",
        "      - debug frames (JPGs with boxes + labels)\n",
        "      - per-frame summaries for GPT\n",
        "\n",
        "    Returns:\n",
        "      df (DataFrame), frame_summaries (list), frames_dir (str), fps (float)\n",
        "    \"\"\"\n",
        "    base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    out_video = f\"{base_name}_annotated.mp4\"\n",
        "    out_csv   = f\"{base_name}_frame_detections.csv\"\n",
        "    frames_dir = f\"{base_name}_frames_debug\"\n",
        "    os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    assert cap.isOpened(), f\"Could not open video: {video_path}\"\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter(out_video, fourcc, fps, (w, h))\n",
        "\n",
        "    detections = []\n",
        "    frame_summaries = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    model_names = None\n",
        "    try:\n",
        "        model_names = model.model.names\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        t = frame_idx / fps\n",
        "\n",
        "        # Skip frames for speed – only run YOLO every `frame_skip` frames\n",
        "        if frame_idx % frame_skip != 0:\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        # YOLO inference with optional class filter\n",
        "        if keep_ids is not None:\n",
        "            results = model(frame, classes=keep_ids)\n",
        "        else:\n",
        "            results = model(frame)\n",
        "\n",
        "        res = results[0]\n",
        "\n",
        "        # annotate BGR frame\n",
        "        annotated = res.plot()\n",
        "        writer.write(annotated)\n",
        "\n",
        "        # get names for classes\n",
        "        names = model_names or getattr(res, \"names\", None)\n",
        "\n",
        "        labels_this = []\n",
        "        label_strings = []\n",
        "\n",
        "        if res.boxes is not None and len(res.boxes) > 0:\n",
        "            boxes_xywh = res.boxes.xywh.cpu().numpy()\n",
        "            clss       = res.boxes.cls.cpu().numpy()\n",
        "            confs      = res.boxes.conf.cpu().numpy()\n",
        "        else:\n",
        "            boxes_xywh = []\n",
        "            clss       = []\n",
        "            confs      = []\n",
        "\n",
        "        for (x, y, w_box, h_box), c, conf in zip(boxes_xywh, clss, confs):\n",
        "            conf = float(conf)\n",
        "            cls_id = int(c)\n",
        "\n",
        "            # resolve label\n",
        "            label = None\n",
        "            if names is not None:\n",
        "                if isinstance(names, dict):\n",
        "                    label = names.get(cls_id, None)\n",
        "                else:\n",
        "                    if 0 <= cls_id < len(names):\n",
        "                        label = names[cls_id]\n",
        "            if label is None:\n",
        "                label = f\"id_{cls_id}\"\n",
        "\n",
        "            # monitor-specific threshold\n",
        "            if label.lower() in MONITOR_LABELS and conf < MONITOR_CONF_THRESH:\n",
        "                continue\n",
        "\n",
        "            # generic threshold for everything else\n",
        "            if conf < conf_thresh:\n",
        "                continue\n",
        "\n",
        "            detections.append({\n",
        "                \"frame\": frame_idx,\n",
        "                \"t_sec\": t,\n",
        "                \"label\": label,\n",
        "                \"conf\": conf,\n",
        "                \"x_center\": float(x),\n",
        "                \"y_center\": float(y),\n",
        "                \"w\": float(w_box),\n",
        "                \"h\": float(h_box),\n",
        "            })\n",
        "\n",
        "            labels_this.append(label)\n",
        "            label_strings.append(\n",
        "                f\"{label}({conf:.2f}) center=({x:.0f},{y:.0f}) size=({w_box:.0f}x{h_box:.0f})\"\n",
        "            )\n",
        "\n",
        "        # per-frame counts and summary text for GPT\n",
        "        counts_this = dict(Counter(labels_this))\n",
        "        frame_summaries.append({\n",
        "            \"frame\": frame_idx,\n",
        "            \"t_sec\": t,\n",
        "            \"text\": \", \".join(label_strings) if label_strings else \"no detections\",\n",
        "            \"counts\": counts_this,\n",
        "        })\n",
        "\n",
        "        # save some debug frames\n",
        "        if frame_idx % save_every == 0:\n",
        "            img_path = os.path.join(frames_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
        "            cv2.imwrite(img_path, annotated)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "    df = pd.DataFrame(detections)\n",
        "    df.to_csv(out_csv, index=False)\n",
        "\n",
        "    print(\"CSV saved:\", out_csv)\n",
        "    print(\"Annotated video saved:\", out_video)\n",
        "    print(\"Debug frames saved in:\", frames_dir)\n",
        "\n",
        "    return df, frame_summaries, frames_dir, fps\n"
      ],
      "metadata": {
        "id": "ZvyWvy13vV8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 – OpenAI GPT client setup\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API key: \")\n",
        "\n",
        "client = OpenAI()\n",
        "print(\"GPT client initialized.\")\n"
      ],
      "metadata": {
        "id": "HnDoHOC_vYMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 – Summarizer + GPT environment classifier (with bounding box positions)\n",
        "\n",
        "def summarize_interval(frame_summaries, t_start, t_end, max_frames=40):\n",
        "    \"\"\"\n",
        "    Build a short text summary + fixed object counts for frames between t_start and t_end.\n",
        "\n",
        "    - Text: up to max_frames lines like:\n",
        "        \"Frame 120 @ 4.0s: chair(0.83) center=(300,200) size=(80x60), table(0.70)...\"\n",
        "    - Counts: for each label, we take the MAX count across frames in the interval.\n",
        "      (avoids exploding totals like \"983 chairs\" from summing every frame.)\n",
        "    \"\"\"\n",
        "    subset = [fs for fs in frame_summaries if t_start <= fs[\"t_sec\"] < t_end]\n",
        "    if not subset:\n",
        "        return None, {}\n",
        "\n",
        "    env_counts = {}\n",
        "    for fs in subset:\n",
        "        for label, c in fs[\"counts\"].items():\n",
        "            env_counts[label] = max(env_counts.get(label, 0), c)\n",
        "\n",
        "    text = \"\\n\".join([\n",
        "        f\"Frame {fs['frame']} @ {fs['t_sec']:.2f}s: {fs['text']}\"\n",
        "        for fs in subset[:max_frames]\n",
        "    ])\n",
        "\n",
        "    return text, env_counts\n",
        "\n",
        "\n",
        "def classify_environment_from_yolo_summary_text(yolo_summary_text, env_counts):\n",
        "    \"\"\"\n",
        "    Call GPT (text-only) to classify the indoor environment using YOLO summaries,\n",
        "    including bounding-box centers and sizes.\n",
        "    \"\"\"\n",
        "    if env_counts:\n",
        "        counts_lines = \"\\n\".join([f\"- {k}: {v}\" for k, v in env_counts.items()])\n",
        "    else:\n",
        "        counts_lines = \"No significant objects detected with sufficient confidence.\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a fine-tuned AI (text only) that classifies INDOOR environments using YOLO object\n",
        "detection text. Each line below describes one video frame and the objects YOLO detected,\n",
        "with confidence scores and bounding-box center/size information.\n",
        "\n",
        "Your tasks:\n",
        "1. Predict the most likely INDOOR environment type (e.g., classroom, office, lab,\n",
        "   hallway, cafeteria, kitchen, lobby, colloquium room, workplace, study hall, bathroom, common area, etc.).\n",
        "2. Explain your reasoning using:\n",
        "   - detected object types\n",
        "   - approximate counts (provided below)\n",
        "   - bounding-box positions (center x/y)\n",
        "   - bounding-box sizes (relative width/height)\n",
        "   - spatial patterns (e.g., rows of chairs, scattered tables, etc.).\n",
        "3. Provide a short list of the main objects with their approximate counts.\n",
        "\n",
        "4.  -If there are many laptops/computers/monitors, weight more towards either a computer lab\n",
        "   -If there are some laptops/computers/monitors, weight more towards either a computer lab or classroom\n",
        "   -If there are tables/chairs in rows, weight more towards a classroom.\n",
        "   -If there is a large number of tables/chairs, especially in clusters, weight more towards an\n",
        "   auditorium or cafeteria.\n",
        "   -Many chairs + couches implies a common area.\n",
        "   -If there are very few objects or mostly empty space, consider a hallway or lobby.\n",
        "   -A sink implies a bathroom unless there are other appliances\n",
        "\n",
        "== YOLO detection summary for this time interval ==\n",
        "{yolo_summary_text}\n",
        "\n",
        "== Estimated object counts for this interval (max per label across frames) ==\n",
        "{counts_lines}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are an assistant that reasons about INDOOR environments \"\n",
        "                    \"from YOLO detection text.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        max_tokens=450,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "Kjikkr1zvay_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 – Run YOLO + GPT every 3 seconds on ONE video\n",
        "\n",
        "def gpt_predictions_every_3s(\n",
        "    model,\n",
        "    video_path,\n",
        "    interval_sec=3.0,\n",
        "    conf_thresh=0.25,\n",
        "):\n",
        "    print(\"\\nRunning YOLO + GPT on:\", video_path)\n",
        "\n",
        "    df, frame_summaries, frames_dir, fps = run_yolo_video(\n",
        "        model,\n",
        "        video_path,\n",
        "        save_every=10,        # save debug frame every 10 processed frames\n",
        "        conf_thresh=conf_thresh,\n",
        "        keep_ids=KEEP_IDS,\n",
        "        frame_skip=5,         # look at every 5th frame\n",
        "    )\n",
        "\n",
        "    duration = df[\"t_sec\"].max() if not df.empty else 0.0\n",
        "    n_intervals = int(math.ceil(duration / interval_sec))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for k in range(n_intervals):\n",
        "        t0 = k * interval_sec\n",
        "        t1 = (k + 1) * interval_sec\n",
        "\n",
        "        interval_text, interval_counts = summarize_interval(frame_summaries, t0, t1)\n",
        "        if not interval_text:\n",
        "            continue\n",
        "\n",
        "        desc = classify_environment_from_yolo_summary_text(interval_text, interval_counts)\n",
        "\n",
        "        print(f\"\\n=== Interval {k} ({t0:.1f}s – {t1:.1f}s) ===\")\n",
        "        print(desc[:700], \"\\n\")\n",
        "\n",
        "        results.append({\n",
        "            \"video\": os.path.basename(video_path),\n",
        "            \"interval_idx\": k,\n",
        "            \"t_start\": t0,\n",
        "            \"t_end\": t1,\n",
        "            \"description\": desc,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "7_HX0U4Mvctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 – Run YOLO + GPT with flexible mode\n",
        "\n",
        "RUN_MODE = \"single\"   # change to \"all\" to run all videos\n",
        "\n",
        "all_interval_preds = []\n",
        "\n",
        "if RUN_MODE == \"single\":\n",
        "    single_video = videos[9]  # choose index manually\n",
        "    print(\"Running only this video:\", single_video)\n",
        "    df_preds = gpt_predictions_every_3s(det_model, single_video, interval_sec=3.0)\n",
        "    all_interval_preds.append(df_preds)\n",
        "\n",
        "else:  # RUN_MODE == \"all\"\n",
        "    print(\"Running ALL videos...\")\n",
        "    for v in videos:\n",
        "        df_preds = gpt_predictions_every_3s(det_model, v, interval_sec=3.0)\n",
        "        all_interval_preds.append(df_preds)\n",
        "\n",
        "# save predictions\n",
        "all_interval_preds = pd.concat(all_interval_preds, ignore_index=True)\n",
        "all_interval_preds.to_csv(\"gpt_interval_predictions.csv\", index=False)\n",
        "\n",
        "print(\"\\nSaved predictions to gpt_interval_predictions.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-DauuD0jvmg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 – Load & peek at interval predictions (works for 1 or many videos)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# If we just ran Cell 7 in this Colab session, reuse all_interval_preds.\n",
        "# Otherwise, fall back to loading from the CSV on disk.\n",
        "if \"all_interval_preds\" in globals() and isinstance(all_interval_preds, pd.DataFrame):\n",
        "    interval_df = all_interval_preds.copy()\n",
        "    print(\"Loaded interval_df from memory. Shape:\", interval_df.shape)\n",
        "else:\n",
        "    interval_df = pd.read_csv(\"gpt_interval_predictions.csv\")\n",
        "    print(\"Loaded interval_df from CSV. Shape:\", interval_df.shape)\n",
        "\n",
        "print(\"\\nSample 3-second predictions:\")\n",
        "print(\n",
        "    interval_df[[\"video\", \"interval_idx\", \"t_start\", \"t_end\", \"description\"]]\n",
        "    .head()\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "a5OKs0jKvpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 – Ground truth per video\n",
        "\n",
        "GROUND_TRUTH = {\n",
        "    \"IMG_1709.MOV\": \"Cafeteria\",\n",
        "    \"IMG_1710.MOV\": \"Cafeteria\",\n",
        "    \"IMG_6404.MOV\": \"Computer Lab\",\n",
        "    \"IMG_6405.MOV\": \"Computer Lab\",\n",
        "    \"IMG_6406.MOV\": \"Hallway\",\n",
        "    \"IMG_6407.MOV\": \"Classroom\",\n",
        "    \"IMG_6409.MOV\": \"Office\",\n",
        "    \"IMG_6410.MOV\": \"Office\",\n",
        "    \"IMG_6411.MOV\": \"Office\",\n",
        "    \"IMG_6412.MOV\": \"Office\",\n",
        "    \"IMG_6413.MOV\": \"Lobby\",\n",
        "    \"IMG_6414.MOV\": \"Study Hall\",\n",
        "    \"IMG_6415.MOV\": \"Study Hall\",\n",
        "    \"IMG_6416.MOV\": \"Colloquium\",\n",
        "    \"IMG_6417.MOV\": \"Colloquium\",\n",
        "    \"IMG_6418.MOV\": \"Dining Area\",\n",
        "    \"IMG_6419.MOV\": \"Study Hall\",\n",
        "    \"IMG_6420.MOV\": \"Kitchen\",\n",
        "    \"IMG_6429.MOV\": \"Classroom\",\n",
        "    \"IMG_6432.MOV\": \"Classroom\",\n",
        "    \"IMG_4049.MOV\": \"Bedroom\",\n",
        "    \"IMG_4051.MOV\": \"Bathroom\",\n",
        "    \"IMG_4052.MOV\": \"Kitchen\",\n",
        "    \"IMG_4055.MOV\": \"Living Room\",\n",
        "    \"IMG_4057.MOV\": \"Bathroom\",\n",
        "}\n",
        "\n",
        "print(\"Ground truth for\", len(GROUND_TRUTH), \"videos loaded.\")\n"
      ],
      "metadata": {
        "id": "Btvm0rAWvrNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 – Canonical environments, synonyms, and label extractor\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# 1) Canonical set of environments we care about\n",
        "CANON_ENVS = [\n",
        "    \"Cafeteria\",\n",
        "    \"Classroom\",\n",
        "    \"Computer Lab\",\n",
        "    \"Dining Area\",\n",
        "    \"Hallway\",\n",
        "    \"Kitchen\",\n",
        "    \"Lobby\",\n",
        "    \"Lounge\",        # instead of \"Living Room\"\n",
        "    \"Office\",\n",
        "    \"Study Hall\",\n",
        "    \"Workplace\",\n",
        "    \"Colloquium\",\n",
        "]\n",
        "\n",
        "# 2) Synonyms / phrases that GPT might use -> map to the canon labels\n",
        "ENV_SYNONYMS = {\n",
        "    # Cafeteria / dining\n",
        "    \"cafeteria\": \"Cafeteria\",\n",
        "    \"cafe\": \"Cafeteria\",\n",
        "    \"café\": \"Cafeteria\",\n",
        "    \"food court\": \"Cafeteria\",\n",
        "\n",
        "    \"dining area\": \"Dining Area\",\n",
        "    \"dining room\": \"Dining Area\",\n",
        "\n",
        "    # Computer lab / lab\n",
        "    \"computer lab\": \"Computer Lab\",\n",
        "    \"lab\": \"Computer Lab\",\n",
        "\n",
        "    # Classroom-ish\n",
        "    \"classroom\": \"Classroom\",\n",
        "    \"lecture hall\": \"Classroom\",\n",
        "\n",
        "    # Hallway\n",
        "    \"hallway\": \"Hallway\",\n",
        "    \"corridor\": \"Hallway\",\n",
        "\n",
        "    # Kitchen\n",
        "    \"kitchen\": \"Kitchen\",\n",
        "\n",
        "    # Lobby / reception\n",
        "    \"lobby\": \"Lobby\",\n",
        "    \"reception\": \"Lobby\",\n",
        "\n",
        "    # Lounge – replaces \"living room\"\n",
        "    \"lounge\": \"Lounge\",\n",
        "    \"lounge area\": \"Lounge\",\n",
        "    \"informal lounge\": \"Lounge\",\n",
        "    \"informal lounge area\": \"Lounge\",\n",
        "    \"living room\": \"Lounge\",   # map living room -> Lounge\n",
        "\n",
        "    # Offices / workspaces\n",
        "    \"office\": \"Office\",\n",
        "    \"workspace\": \"Workplace\",\n",
        "    \"workplace\": \"Workplace\",\n",
        "\n",
        "    # Study spaces\n",
        "    \"study hall\": \"Study Hall\",\n",
        "    \"study space\": \"Study Hall\",\n",
        "    \"study area\": \"Study Hall\",\n",
        "\n",
        "    # Colloquium\n",
        "    \"colloquium room\": \"Colloquium\",\n",
        "    \"colloquium\": \"Colloquium\",\n",
        "}\n",
        "\n",
        "\n",
        "def extract_env_label(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Look through a GPT description, count how many times each environment\n",
        "    (via its synonyms) appears, and return the best-matching canonical env.\n",
        "    If nothing matches, returns 'UNKNOWN'.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"UNKNOWN\"\n",
        "\n",
        "    t = text.lower()\n",
        "    scores = {env: 0 for env in CANON_ENVS}\n",
        "\n",
        "    for phrase, env in ENV_SYNONYMS.items():\n",
        "        count = t.count(phrase)\n",
        "        if count > 0:\n",
        "            scores[env] += count\n",
        "\n",
        "    best_env = \"UNKNOWN\"\n",
        "    best_score = 0\n",
        "    for env, s in scores.items():\n",
        "        if s > best_score:\n",
        "            best_env, best_score = env, s\n",
        "\n",
        "    return best_env if best_score > 0 else \"UNKNOWN\"\n"
      ],
      "metadata": {
        "id": "-AKzxBWevtXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 – Interval-level + video-level accuracy & confusion matrices\n",
        "\n",
        "# ---- 1) Add predicted + true environment labels to each 3-second window ----\n",
        "interval_df = interval_df.copy()\n",
        "\n",
        "interval_df[\"pred_env\"] = interval_df[\"description\"].apply(extract_env_label)\n",
        "interval_df[\"true_env\"] = interval_df[\"video\"].map(GROUND_TRUTH)\n",
        "\n",
        "print(\"Sample 3-second predictions:\")\n",
        "print(\n",
        "    interval_df[[\"video\", \"interval_idx\", \"t_start\", \"t_end\", \"true_env\", \"pred_env\"]]\n",
        "    .head()\n",
        ")\n",
        "\n",
        "# Keep only rows where we know the ground-truth env\n",
        "valid_int = interval_df.dropna(subset=[\"true_env\"])\n",
        "\n",
        "# -------- Interval-level accuracy --------\n",
        "acc_int = accuracy_score(valid_int[\"true_env\"], valid_int[\"pred_env\"])\n",
        "print(\"\\nInterval-level overall accuracy (3-sec windows):\", acc_int)\n",
        "\n",
        "# Use only envs that actually appear in either true or predicted labels\n",
        "used_int_labels = sorted(\n",
        "    set(valid_int[\"true_env\"]) | set(valid_int[\"pred_env\"])\n",
        ")\n",
        "cm_int = confusion_matrix(\n",
        "    valid_int[\"true_env\"],\n",
        "    valid_int[\"pred_env\"],\n",
        "    labels=used_int_labels,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm_int,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=used_int_labels,\n",
        "    yticklabels=used_int_labels,\n",
        ")\n",
        "plt.xlabel(\"Predicted environment\")\n",
        "plt.ylabel(\"True environment\")\n",
        "plt.title(\"Interval-level Confusion Matrix (all 3-second predictions)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------- 2) Video-level majority vote over intervals --------\n",
        "video_env = (\n",
        "    valid_int\n",
        "    .groupby(\"video\")[\"pred_env\"]\n",
        "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else CANON_ENVS[0])\n",
        "    .reset_index()\n",
        "    .rename(columns={\"video\": \"basename\"})\n",
        ")\n",
        "\n",
        "video_env[\"true_env\"] = video_env[\"basename\"].map(GROUND_TRUTH)\n",
        "\n",
        "print(\"\\nVideo-level predictions (majority over 3-second intervals):\")\n",
        "print(video_env)\n",
        "\n",
        "valid_videos = video_env.dropna(subset=[\"true_env\"])\n",
        "video_acc = accuracy_score(valid_videos[\"true_env\"], valid_videos[\"pred_env\"])\n",
        "print(\"\\nVideo-level overall accuracy:\", video_acc)\n",
        "\n",
        "used_video_labels = sorted(\n",
        "    set(valid_videos[\"true_env\"]) | set(valid_videos[\"pred_env\"])\n",
        ")\n",
        "\n",
        "cm_video = confusion_matrix(\n",
        "    valid_videos[\"true_env\"],\n",
        "    valid_videos[\"pred_env\"],\n",
        "    labels=used_video_labels,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm_video,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=used_video_labels,\n",
        "    yticklabels=used_video_labels,\n",
        ")\n",
        "plt.xlabel(\"Predicted environment\")\n",
        "plt.ylabel(\"True environment\")\n",
        "plt.title(\"Video-level Confusion Matrix (majority over 3-second intervals)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m7GBjEU6vzE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12 – Display some annotated debug frames\n",
        "\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_frames(frames_dir, max_frames=12):\n",
        "    files = sorted(glob.glob(os.path.join(frames_dir, \"*.jpg\")))[:max_frames]\n",
        "    if not files:\n",
        "        print(\"No frames found in\", frames_dir)\n",
        "        return\n",
        "\n",
        "    cols = 4\n",
        "    rows = (len(files) + cols - 1) // cols\n",
        "\n",
        "    plt.figure(figsize=(20, 4 * rows))\n",
        "    for i, f in enumerate(files):\n",
        "        img = cv2.imread(f)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(os.path.basename(f))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example: display frames from first video\n",
        "if videos:\n",
        "    first = os.path.splitext(os.path.basename(videos[0]))[0] + \"_frames_debug\"\n",
        "    display_frames(first, max_frames=12)\n",
        "else:\n",
        "    print(\"No videos available.\")\n"
      ],
      "metadata": {
        "id": "nCWC0XG3v2Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13 – GIF creation\n",
        "\n",
        "import numpy as np\n",
        "import imageio\n",
        "from IPython.display import Image as IImage, display as idisplay\n",
        "\n",
        "def make_gif(frames_dir, gif_path=\"annotated.gif\", fps=10):\n",
        "    files = sorted(glob.glob(os.path.join(frames_dir, \"*.jpg\")))\n",
        "    if not files:\n",
        "        print(\"No frames found.\")\n",
        "        return\n",
        "\n",
        "    base = imageio.imread(files[0])\n",
        "    H, W = base.shape[:2]\n",
        "\n",
        "    frames = []\n",
        "    for f in files:\n",
        "        img = imageio.imread(f)\n",
        "        img_resized = cv2.resize(img, (W, H))\n",
        "        frames.append(img_resized)\n",
        "\n",
        "    imageio.mimsave(gif_path, frames, duration=1 / fps)\n",
        "    print(\"GIF created:\", gif_path)\n",
        "    idisplay(IImage(filename=gif_path))\n",
        "\n",
        "# Example\n",
        "if videos:\n",
        "    first = os.path.splitext(os.path.basename(videos[0]))[0] + \"_frames_debug\"\n",
        "    make_gif(first, \"annotated.gif\", fps=10)\n",
        "else:\n",
        "    print(\"No videos available.\")\n"
      ],
      "metadata": {
        "id": "De8W_QCvv4s_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}