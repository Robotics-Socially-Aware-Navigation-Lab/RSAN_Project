{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBGAU4NQQfO9",
        "outputId": "d47b6f6e-f0d4-45c9-f629-fac0bf708e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found Places365 labels at: categories_places365.txt\n",
            "Loaded 365 Places365 class names.\n",
            "Allowed Places365 classes:\n",
            "   27 -> auditorium\n",
            "   38 -> banquet_hall\n",
            "   45 -> bathroom\n",
            "   52 -> bedroom\n",
            "   75 -> cafeteria\n",
            "   92 -> classroom\n",
            "  100 -> computer_room\n",
            "  102 -> conference_room\n",
            "  106 -> corridor\n",
            "  120 -> dining_hall\n",
            "  121 -> dining_room\n",
            "  203 -> kitchen\n",
            "  215 -> living_room\n",
            "  217 -> lobby\n",
            "  244 -> office\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# VIDEO ENVIRONMENT PREDICTION (PLACES365) EVERY ~3 SECONDS\n",
        "# ---------------------------------------------------------------\n",
        "# - Loads Places365 labels for readability\n",
        "# - Loads your fine-tuned multi-head model from LOCAL paths\n",
        "# - Samples frames from a video about every 3 seconds\n",
        "# - Prints human-readable Places365 predictions over time\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG: EDIT THESE PATHS\n",
        "# ----------------------------\n",
        "\n",
        "# Local path to YOUR fine-tuned multi-head model (.pth)\n",
        "FINETUNED_MODEL_PATH = Path(\"/content/resnet_places365_best.pth\")  # e.g. /content/resnet_places365_mit_multihead_best.pth\n",
        "\n",
        "# Local Places365 label file (will auto-download if missing)\n",
        "PLACES_LABELS_PATH = Path(\"categories_places365.txt\")\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 1: Ensure Places365 label file\n",
        "# ----------------------------\n",
        "\n",
        "def download_places365_labels_if_needed(labels_path: Path):\n",
        "    \"\"\"\n",
        "    Download the official Places365 categories file if it doesn't exist.\n",
        "    \"\"\"\n",
        "    if labels_path.exists():\n",
        "        print(\"Found Places365 labels at:\", labels_path)\n",
        "        return\n",
        "\n",
        "    print(\"Downloading Places365 label file...\")\n",
        "    os.system(\n",
        "        f\"wget -O '{labels_path}' \"\n",
        "        \"https://raw.githubusercontent.com/CSAILVision/places365/master/categories_places365.txt\"\n",
        "    )\n",
        "    if not labels_path.exists():\n",
        "        raise FileNotFoundError(\"Failed to download categories_places365.txt\")\n",
        "\n",
        "\n",
        "def load_places365_labels(labels_path: Path):\n",
        "    \"\"\"\n",
        "    Load Places365 class names in correct index order from a file that looks like:\n",
        "      /a/abbey 0\n",
        "      /b/beach 48\n",
        "      /k/kitchen 123\n",
        "    We turn those into plain names like 'abbey', 'beach', 'kitchen', indexed by ID.\n",
        "    \"\"\"\n",
        "    num_classes = 365\n",
        "    classes = [None] * num_classes\n",
        "\n",
        "    with open(labels_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "\n",
        "            category_full = parts[0]       # e.g. '/a/abbey' or 'abbey'\n",
        "            try:\n",
        "                cls_id = int(parts[-1])    # the last token is the ID\n",
        "            except ValueError:\n",
        "                # If the file somehow doesn't end with an int, skip\n",
        "                continue\n",
        "\n",
        "            # Human-readable name: last part after '/'\n",
        "            # '/a/abbey' -> 'abbey'\n",
        "            # 'abbey'    -> 'abbey'\n",
        "            name = category_full.split(\"/\")[-1]\n",
        "\n",
        "            if 0 <= cls_id < num_classes:\n",
        "                classes[cls_id] = name\n",
        "\n",
        "    # Fill any missing entries with a fallback\n",
        "    for i in range(num_classes):\n",
        "        if classes[i] is None:\n",
        "            classes[i] = f\"class_{i}\"\n",
        "\n",
        "    return classes\n",
        "\n",
        "\n",
        "\n",
        "download_places365_labels_if_needed(PLACES_LABELS_PATH)\n",
        "places_class_names = load_places365_labels(PLACES_LABELS_PATH)\n",
        "print(\"Loaded\", len(places_class_names), \"Places365 class names.\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 2: Model definitions\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "class PlacesMITMultiHead(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet50 backbone with:\n",
        "      - places_head: 365 Places365 classes\n",
        "      - mit_head: N indoor classes\n",
        "\n",
        "    NOTE: This version does NOT take a pretrained resnet as an argument.\n",
        "          We just build the architecture here and then load your fine-tuned\n",
        "          state_dict from FINETUNED_MODEL_PATH.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_mit_classes: int):\n",
        "        super().__init__()\n",
        "        # Base ResNet50 (random init; we will overwrite with state_dict)\n",
        "        self.backbone = models.resnet50(num_classes=365)\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "\n",
        "        # Places365 head (365 classes)\n",
        "        self.places_head = nn.Linear(in_features, 365)\n",
        "\n",
        "        # Backbone outputs features instead of logits\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # MIT indoor head (your classes)\n",
        "        self.mit_head = nn.Linear(in_features, num_mit_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        places_logits = self.places_head(feats)\n",
        "        mit_logits = self.mit_head(feats)\n",
        "        return places_logits, mit_logits\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 3: Image transform (no augmentation)\n",
        "# ----------------------------\n",
        "\n",
        "video_tf = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 4: Build model & load your weights\n",
        "# ----------------------------\n",
        "\n",
        "def build_finetuned_model(finetuned_path: Path, num_mit_classes: int):\n",
        "    \"\"\"\n",
        "    Build multi-head model and load your fine-tuned weights from local .pth.\n",
        "    NO original Places365 .pth.tar needed at inference.\n",
        "    \"\"\"\n",
        "    print(\"Building multi-head model architecture...\")\n",
        "    model = PlacesMITMultiHead(num_mit_classes=num_mit_classes)\n",
        "\n",
        "    if not finetuned_path.exists():\n",
        "        raise FileNotFoundError(f\"Fine-tuned model not found at {finetuned_path}\")\n",
        "\n",
        "    print(\"Loading fine-tuned weights from:\", finetuned_path)\n",
        "    state = torch.load(str(finetuned_path), map_location=DEVICE)\n",
        "    model.load_state_dict(state)\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print(\"Model ready for video inference.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 5: Video classification function\n",
        "# ----------------------------\n",
        "# --------------------------------------------------\n",
        "# Build a list of indices for the Places classes we care about\n",
        "# --------------------------------------------------\n",
        "\n",
        "# MIT indoor class names (update if your list is different)\n",
        "\n",
        "MIT_CLASS_NAMES = [ #ORDER MATTERS - must be in this order (only change if u wanna rename classes)\n",
        "    \"bathroom\",\n",
        "    \"bedroom\",\n",
        "    \"classroom\",\n",
        "    \"colloquium\",\n",
        "    \"common_area\",\n",
        "    \"computer_lab\",\n",
        "    \"hallway\",\n",
        "    \"kitchen\",\n",
        "    \"library\",\n",
        "    \"living_room\",\n",
        "    \"office\",\n",
        "]\n",
        "\n",
        "\n",
        "NUM_MIT_CLASSES = len(MIT_CLASS_NAMES)\n",
        "\n",
        "\n",
        "\n",
        "ALLOWED_PLACES_CLASSES = [ #this order does not matter, just make sure it exists in places365 (github)\n",
        "    \"office\",\n",
        "    \"corridor\",\n",
        "    \"classroom\",\n",
        "    \"kitchen\",\n",
        "    \"bathroom\",\n",
        "    \"library\",\n",
        "    \"living_room\",\n",
        "    \"dining_room\",\n",
        "    \"computer_room\",\n",
        "    \"cafeteria\",\n",
        "    \"lobby\",\n",
        "    \"auditorium\",\n",
        "    \"banquet_hall\",\n",
        "    \"library/indoor\",\n",
        "    \"bedroom\",\n",
        "    \"church/indoor\",\n",
        "    \"conference_room\",\n",
        "    \"dining_hall\",\n",
        "    \"garage/indoor\",\n",
        "]\n",
        "\n",
        "allowed_places_indices = []\n",
        "for i, name in enumerate(places_class_names):\n",
        "    if name in ALLOWED_PLACES_CLASSES:\n",
        "        allowed_places_indices.append(i)\n",
        "\n",
        "print(\"Allowed Places365 classes:\")\n",
        "for idx in allowed_places_indices:\n",
        "    print(f\"  {idx:3d} -> {places_class_names[idx]}\")\n",
        "\n",
        "if not allowed_places_indices:\n",
        "    print(\"⚠️ Warning: no allowed classes matched! Check spelling.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify_video_both_heads_fast(\n",
        "    video_path: Path,\n",
        "    model: nn.Module,\n",
        "    step_seconds: float = 3.0,\n",
        "    topk_places: int = 3,\n",
        "    topk_mit: int = 3,\n",
        "    mit_priority_threshold: float = 0.6,\n",
        "    show_timestamps: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Classify video using BOTH Places365 head and MIT indoor head.\n",
        "\n",
        "    - Samples a frame every `step_seconds`\n",
        "    - Runs both heads\n",
        "    - Chooses an \"overall best\" label:\n",
        "        * If MIT top-1 prob >= mit_priority_threshold -> use MIT label\n",
        "        * else -> use Places365 label\n",
        "    - Prints:\n",
        "        * Overall best label\n",
        "        * Top MIT predictions\n",
        "        * Top Places predictions\n",
        "    \"\"\"\n",
        "    if not video_path.exists():\n",
        "        raise FileNotFoundError(f\"Video not found at {video_path}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps <= 0:\n",
        "        print(\"Warning: could not read FPS, defaulting to 30.\")\n",
        "        fps = 30.0\n",
        "\n",
        "    frame_interval = int(fps * step_seconds)\n",
        "    if frame_interval <= 0:\n",
        "        frame_interval = 1\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"\\nVideo: {video_path}\")\n",
        "    print(\n",
        "        f\"FPS: {fps:.2f}, total_frames: {total_frames}, \"\n",
        "        f\"sampling every {frame_interval} frames (~{step_seconds}s)\\n\"\n",
        "    )\n",
        "\n",
        "    sampled_idx = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    while frame_idx < total_frames:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame_bgr = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        sampled_idx += 1\n",
        "\n",
        "        # BGR -> RGB\n",
        "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(frame_rgb)\n",
        "\n",
        "        # Preprocess\n",
        "        x = video_tf(pil_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "        # Forward pass: BOTH heads\n",
        "        with torch.no_grad():\n",
        "            places_logits, mit_logits = model(x)\n",
        "            places_probs = F.softmax(places_logits, dim=1)[0]\n",
        "            mit_probs    = F.softmax(mit_logits, dim=1)[0]\n",
        "\n",
        "\n",
        "        # ----- Places365 top-k -----\n",
        "        if allowed_places_indices:\n",
        "            subset_probs = places_probs[allowed_places_indices]\n",
        "            p_topk_probs, p_topk_idx = torch.topk(\n",
        "                subset_probs,\n",
        "                k=min(topk_places, len(allowed_places_indices)),\n",
        "            )\n",
        "            p_topk_probs = p_topk_probs.cpu().numpy()\n",
        "            p_topk_idx   = p_topk_idx.cpu().numpy()\n",
        "            p_global_idx = [allowed_places_indices[i] for i in p_topk_idx]\n",
        "        else:\n",
        "            p_topk_probs, p_global_idx = torch.topk(places_probs, k=topk_places)\n",
        "            p_topk_probs = p_topk_probs.cpu().numpy()\n",
        "            p_global_idx = p_global_idx.cpu().numpy()\n",
        "\n",
        "        # Top-1 Places\n",
        "        places_best_idx = int(p_global_idx[0])\n",
        "        places_best_prob = float(p_topk_probs[0])\n",
        "        places_best_name = places_class_names[places_best_idx]\n",
        "\n",
        "        # ----- MIT head top-k -----\n",
        "        m_topk_probs, m_topk_idx = torch.topk(\n",
        "            mit_probs,\n",
        "            k=min(topk_mit, len(MIT_CLASS_NAMES)),\n",
        "        )\n",
        "\n",
        "        m_topk_probs = m_topk_probs.cpu().numpy()\n",
        "        m_topk_idx   = m_topk_idx.cpu().numpy()\n",
        "        m_topk_names = [MIT_CLASS_NAMES[i] for i in m_topk_idx]\n",
        "\n",
        "        # Top-1 MIT\n",
        "        mit_best_idx = int(m_topk_idx[0])\n",
        "        mit_best_prob = float(m_topk_probs[0])\n",
        "        mit_best_name = MIT_CLASS_NAMES[mit_best_idx]\n",
        "\n",
        "        # ----- Choose \"overall best\" -----\n",
        "        # If MIT is very confident (e.g. indoor bathroom, classroom, etc.), trust it.\n",
        "        # Otherwise, fall back to Places365's best guess.\n",
        "        # ----- Choose \"overall best\" -----\n",
        "        # Special case: if Places365 top-1 is 'cafeteria', ALWAYS trust it.\n",
        "\n",
        "\n",
        "        if places_best_name == \"cafeteria\":\n",
        "            overall_source = \"Places365\"\n",
        "            overall_name   = places_best_name\n",
        "            overall_prob   = places_best_prob\n",
        "\n",
        "        # Otherwise, use the normal MIT-priority rule\n",
        "        elif mit_best_prob >= mit_priority_threshold:\n",
        "            overall_source = \"MIT\"\n",
        "            overall_name   = mit_best_name\n",
        "            overall_prob   = mit_best_prob\n",
        "        else:\n",
        "            overall_source = \"Places365\"\n",
        "            overall_name   = places_best_name\n",
        "            overall_prob   = places_best_prob\n",
        "\n",
        "        t_sec = frame_idx / fps if fps > 0 else frame_idx / 30.0\n",
        "\n",
        "        print(\"---------------------------------------------------\")\n",
        "        if show_timestamps:\n",
        "            print(f\"Sample #{sampled_idx} at ~{t_sec:.1f}s:\")\n",
        "\n",
        "        print(f\"OVERALL BEST: [{overall_source}] {overall_name}  prob={overall_prob:.3f}\")\n",
        "\n",
        "        print(\"\\nMIT head (fine-tuned) top predictions:\")\n",
        "        for name, p in zip(m_topk_names, m_topk_probs):\n",
        "            print(f\"  {name:20s} prob={p:.3f}\")\n",
        "\n",
        "        print(\"\\nPlaces365 head (original) top predictions:\")\n",
        "        for cls_idx, p in zip(p_global_idx, p_topk_probs):\n",
        "            cls_idx = int(cls_idx)\n",
        "            cls_name = places_class_names[cls_idx] if 0 <= cls_idx < len(places_class_names) else f\"idx_{cls_idx}\"\n",
        "            print(f\"  {cls_name:30s} prob={p:.3f}\")\n",
        "\n",
        "        frame_idx += frame_interval\n",
        "\n",
        "    cap.release()\n",
        "    print(\"\\nDone processing video.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# STEP 6: EXAMPLE USAGE\n",
        "# ----------------------------\n",
        "\n",
        "# 1) Specify how many MIT classes you trained with (e.g., 10)\n",
        "NUM_MIT_CLASSES = 11\n",
        "\n",
        "# 2) Build model once\n",
        "video_model = build_finetuned_model(FINETUNED_MODEL_PATH, num_mit_classes=NUM_MIT_CLASSES)\n",
        "\n",
        "# 3) Run on a local video file (edit this path!)\n",
        "example_video_path = Path(\"/content/Auditorium.MOV\")  # e.g. /content/example_video.mp4\n",
        "\n",
        "# Uncomment to run:\n",
        "classify_video_both_heads_fast(\n",
        "    example_video_path,\n",
        "    video_model,\n",
        "    step_seconds=2.0,   # every X seconds\n",
        "    topk_places=2,  #display lower condidence predictions\n",
        "    topk_mit=2,     #display lower condidence predictions\n",
        "    mit_priority_threshold=0.6, #how low to allow mit head to predict before it switches to places365\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyptILhzRV3G",
        "outputId": "8afe39be-5c1b-4ccd-9d45-687c9df845ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building multi-head model architecture...\n",
            "Loading fine-tuned weights from: /content/resnet_places365_best.pth\n",
            "Model ready for video inference.\n",
            "\n",
            "Video: /content/Auditorium.MOV\n",
            "FPS: 59.94, total_frames: 321, sampling every 119 frames (~2.0s)\n",
            "\n",
            "---------------------------------------------------\n",
            "Sample #1 at ~0.0s:\n",
            "OVERALL BEST: [MIT] colloquium  prob=0.998\n",
            "\n",
            "MIT head (fine-tuned) top predictions:\n",
            "  colloquium           prob=0.998\n",
            "  library              prob=0.001\n",
            "\n",
            "Places365 head (original) top predictions:\n",
            "  auditorium                     prob=0.050\n",
            "  conference_room                prob=0.001\n",
            "---------------------------------------------------\n",
            "Sample #2 at ~2.0s:\n",
            "OVERALL BEST: [MIT] colloquium  prob=0.998\n",
            "\n",
            "MIT head (fine-tuned) top predictions:\n",
            "  colloquium           prob=0.998\n",
            "  classroom            prob=0.001\n",
            "\n",
            "Places365 head (original) top predictions:\n",
            "  auditorium                     prob=0.052\n",
            "  conference_room                prob=0.001\n",
            "---------------------------------------------------\n",
            "Sample #3 at ~4.0s:\n",
            "OVERALL BEST: [MIT] colloquium  prob=0.998\n",
            "\n",
            "MIT head (fine-tuned) top predictions:\n",
            "  colloquium           prob=0.998\n",
            "  classroom            prob=0.001\n",
            "\n",
            "Places365 head (original) top predictions:\n",
            "  auditorium                     prob=0.096\n",
            "  conference_room                prob=0.001\n",
            "\n",
            "Done processing video.\n"
          ]
        }
      ]
    }
  ]
}